import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

# --- ×”×’×“×¨×•×ª ---
START_URL = "https://www.Maasecha.com/"
# ---------------------------------------------

# ××©×ª× ×™× ×’×œ×•×‘×œ×™×™× ×œ××—×¡×•×Ÿ ×”× ×ª×•× ×™×
all_internal_urls = set()
all_external_urls = set()
broken_links = {}

def is_valid(url):
    """
    ×‘×•×“×§ ×× ×”×§×™×©×•×¨ ×”×•× ×ª×§×™×Ÿ (×œ× ×§×™×©×•×¨ ×œ××™×™×œ, ×˜×œ×¤×•×Ÿ ×•×›×•')
    """
    parsed = urlparse(url)
    return bool(parsed.netloc) and bool(parsed.scheme)

def check_link_status(url, source_page):
    """
    ×‘×•×“×§ ××ª ×”×¡×˜×˜×•×¡ ×©×œ ×§×™×©×•×¨ ×‘×•×“×“
    """
    try:
        response = requests.head(url, timeout=10)
        if response.status_code >= 400:
            broken_links[url] = {"source": source_page, "status": response.status_code}
            print(f"âŒ ×§×™×©×•×¨ ×©×‘×•×¨ × ××¦×: {url} (×¡×˜×˜×•×¡ {response.status_code}) ×‘×¢××•×“: {source_page}")
    except requests.RequestException as e:
        broken_links[url] = {"source": source_page, "status": "×©×’×™××ª ×¨×©×ª"}
        print(f"âŒ ×§×™×©×•×¨ ×©×‘×•×¨ × ××¦×: {url} (×©×’×™××ª ×¨×©×ª) ×‘×¢××•×“: {source_page}")

def crawl(url, base_domain):
    """
    ×”×¤×•× ×§×¦×™×” ×”××¨×›×–×™×ª ×©×¡×•×¨×§×ª ×“×£, ××•×¦××ª ×§×™×©×•×¨×™× ×•××¤×¢×™×œ×” ××ª ×¢×¦××” ××—×“×© ×¢×œ ×§×™×©×•×¨×™× ×¤× ×™××™×™×
    """
    if url in all_internal_urls:
        return 
    
    print(f"ğŸ” ×¡×•×¨×§ ××ª ×”×¢××•×“: {url}")
    all_internal_urls.add(url)
    
    try:
        response = requests.get(url, timeout=10)
    except requests.RequestException:
        return

    soup = BeautifulSoup(response.content, 'html.parser')

    for a_tag in soup.find_all("a", href=True):
        href = a_tag.attrs.get("href")
        if not href:
            continue
            
        full_url = urljoin(url, href)
        
        if not is_valid(full_url):
            continue

        # ×‘×“×™×§×ª ×”×§×™×©×•×¨ ×‘×¤×¢× ×”×¨××©×•× ×” ×©×¤×•×’×©×™× ××•×ª×•
        if full_url not in broken_links and full_url not in all_external_urls and full_url not in all_internal_urls:
             check_link_status(full_url, url)

        if urlparse(full_url).netloc != base_domain:
            if full_url not in all_external_urls:
                all_external_urls.add(full_url)
        else:
            if full_url not in all_internal_urls:
                crawl(full_url, base_domain)


def main():
    base_domain = urlparse(START_URL).netloc
    print(f"--- ××ª×—×™×œ ×¡×¨×™×§×ª ×§×™×©×•×¨×™× ×‘××ª×¨: {base_domain} ---")
    
    crawl(START_URL, base_domain)

    print("\n\n--- ×¡×¨×™×§×” ×”×•×©×œ××”. ××›×™×Ÿ ×“×•×— ---")
    
    # --- ×“×•×— ×§×™×©×•×¨×™× ×©×‘×•×¨×™× ---
    if broken_links:
        print(f"\nâŒ × ××¦××• {len(broken_links)} ×§×™×©×•×¨×™× ×©×‘×•×¨×™×:")
        for link, details in broken_links.items():
            print(f"  - ×§×™×©×•×¨: {link} | ×¡×˜×˜×•×¡: {details['status']} | × ××¦× ×‘×¢××•×“: {details['source']}")
    else:
        print("\nâœ… ×›×œ ×”×›×‘×•×“! ×œ× × ××¦××• ×§×™×©×•×¨×™× ×©×‘×•×¨×™× ×‘××ª×¨.")
        
    # --- ×“×•×— ×›×œ×œ×™ (×¡×™×›×•×) ---
    print("\n" + "="*40)
    print("ğŸ“Š ×¡×™×›×•× ×›×œ×œ×™:")
    print(f"   - × ×¡×¨×§×• {len(all_internal_urls)} ×“×¤×™× ×¤× ×™××™×™×.")
    print(f"   - × ×‘×“×§×• {len(all_external_urls)} ×§×™×©×•×¨×™× ×—×™×¦×•× ×™×™×.")
    print(f"   - × ××¦××• {len(broken_links)} ×§×™×©×•×¨×™× ×©×‘×•×¨×™×.")
    print("="*40)
    
    # --- ×”×•×¡×¤×” ×—×“×©×”: ×”×“×¤×¡×ª ×›×œ ×”×§×™×©×•×¨×™× ×©× ××¦××• ---
    print("\n×¨×©×™××ª ×›×œ ×”×“×¤×™× ×”×¤× ×™××™×™× ×©× ×¡×¨×§×•:")
    for internal_url in sorted(list(all_internal_urls)):
        print(f"  - {internal_url}")
        
    print("\n×¨×©×™××ª ×›×œ ×”×§×™×©×•×¨×™× ×”×—×™×¦×•× ×™×™× ×©× ××¦××•:")
    if all_external_urls:
        for external_url in sorted(list(all_external_urls)):
            print(f"  - {external_url}")
    else:
        print("  - ×œ× × ××¦××• ×§×™×©×•×¨×™× ×—×™×¦×•× ×™×™×.")


if __name__ == "__main__":
    main()